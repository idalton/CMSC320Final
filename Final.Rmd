---
title: "Sweet Learning: Exploring the Data Science Pipeline with Honey"
author: "Ian Dalton"
date: "May 18, 2018"
output: 
  html_document: 
    toc: yes
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = file.path(dirname(inputFile), 'index.html')) })
citecolor: gold
urlcolor: gold
---

```{r libs, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE, warning=FALSE,fig.width=12, fig.height=7)

library(data.table)
library(dplyr)
library(ggplot2)
library(stringr)
library(DT)
library(tidyr)
library(readr)
library(leaflet)
library(lubridate)
```




walk users through the entire data science pipeline:  
- data curation,  
- parsing, and management  
- exploratory data analysis  
- hypothesis testing and
- machine learning to provide analysis  
- and then the curation of a message or messages covering insights learned during the tutorial.  


## Introduction  
Do you like Bees? I dont like bees, but I do like honey and bee plant pollination. Bees are incredibly important for agriculture because the plant pollination they provide. A large portion of the food people eat comes from plants that have been pollinated by bees. Without the pollination bees provide, food producing plants In recent years bee colonies were in decline for a number of mysterious reasons. Things seem to be improving and there are significant efforts to solve the problems dealing with the decline. If you are interested in learning more you can do so [here](https://thehoneybeeconservancy.org/2017/06/22/honey-bees-heroes-planet/).
The current bee situation prevents an interesting oppurtunity to dive into the data science pipeline with Honey data taken from the National Agricultural Statistics Service.

In this tutorial I will guide you through: scraping honey production data N.A.S.S., making plots to help visualize the data, analyze the size of bee colonies to see if bees how bee populations are changing over the years, and make a predictor using a random forest to see if we can predict whether the number of bees will increase or decrease in the next years.

**DATA**  
I first discovered this data while browsing kaggle. The kaggle dataset, found [here](https://www.kaggle.com/jessicali9530/honey-production), had honey production data recorded from the National Agricultural Statistics Service.

The dataset included data from 1998 to 2012.    
I found that the dataset was created from a series of `csv` files contained within zip files for years 1998-2012.   However the National Agricultural Statistics Service website has data from 1986 and onward in text files.  
I thought this would be a good oppurtunity showcase the data scraping and tidying parts of the data science pipeline. This will also expand the amount of data we have available for analysis later.

## Data Scraping and Tidying
I downloaded all the text documents locally and gave them descriptive names in order to use them easily in my R code I have included the text files in this projects [github repository](https://github.com/idalton/CMSC320Final/tree/gh-pages).  
I encourage you to look at the text file links below to get a better understanding of the format of the text files and to see what we are working with when we scrape the data from them.  

**links:**  
[HoneyData_1986-1992.txt](./HoneyData_1986-1992.txt)  
[HoneyData_1993-1997.txt](./HoneyData_1993-1997.txt)  
[HoneyData_1998-2002.txt](./HoneyData_1998-2002.txt)  
[HoneyData_2003-2007.txt](./HoneyData_2003-2007.txt)  
[HoneyData_2008-2012.txt](./HoneyData_2008-2012.txt)  



first some setup code  
Here I extracted the columns from the tables in the text files. I make a vector of the abbreviated names which i can then use as column names when i create the data frames from the scraped table data. The data as it stands now in the tables are in varying aggregate units (ie 1,000's) we will fix this when we create the data frames  

also first four files have state abbreviations in place of full statenames. This Is fine and we will use the abreviations for some plots when we make use of pl
 
 
```{r setup}

# Column names taken from the text files
headers <- c(
  "state", # abreviations
	"numcolonies", # 1,000s
	"yieldpercol", # in pounds
	"production",# in 1000 pounds
	"stocks", # 1,000 Pounds
	"avgprice", # cents
	"value" #$1,000s
  )  


abbr_to_name <- setNames(state.name,state.abb)

```


You may notice that the text format of the data in the `HoneyData_2008-2012.txt` file is slightly different from that of the other files. We will need to address this later by changing how we scrape the data from that file in particular.

Also each of the tables have rows that contain totals at the bottom. I will not be taking the totals data from the files because I only want individual state data from each year. Additionally some states are not listed individually and get clumped together in a `Oth Sts` row.

I will include this missing data by scraping the `Oth Sts` row and dividing it evenly amoung the states it represents. This way i will have some data from each state for each year.

I want data for all 50 states and there is data from every state except Alaska which I will include into the the table as all zeros so that i can have data for each state for each year

For now, I will not be including this data. I only want to use state specific data and I dont think there is much benefit in taking the effort to include this data.


`TODO`
what to do about HI and AK alaska
only contigous 48?
aka average

**Scraping Data from 1986-2007**  

For the most part, the first four files that deal with years 1986-2007 have the same format so we can reuse the code to scrape the data from them. I made a function that will scrape the data when given the text information of the files

A couple notes about the file format and how I approached scraping the data before I get to the code:
- All the tables start with data from Alabama (AL)
- All the tables end with data from Wyoming (WY)
- the table order corresponds to the year order for each file  

The `gethoneydatav1`is the data scraping function (`v1` because we will need `v2` for the last file).
Given the output of `read_lines`, it finds all the start and end indices of each table, creates a string vector of all the lines of the table, makes each entry space delimited then adds it to a dataframe. The return value of the function is a dataframe that contains the data from each table in the file with an added column indicating the year for that data. The function also takes a `year` value which stands in as the base number to add onto the index to create the year colunms.

```{r GetDataFuntion}

alaska <- data_frame() %>%
  rbind(c("AK",rep("0",7))) %>%
  setNames(headers)
# names(alaska) <- headers
gethoneydatav1 <- function(text,year) {
   # there is a space there because apparently one of the files has
  # the string "AL" which was messing up the grep search
  start_lines <- c(grep("AL ",text))
  # I included the space here as well just to be safe
  end_lines <- c(grep("WY ",text))
  othersts_lines <- c(grep("Sts",text))  
  
  df = NULL # intializing the accumlator data frame
  # for each table found in the text file
  for (index in 1:length(start_lines)){
    
    # since we have index bounds of the table we can grab the lines that make up the table
    table <- text[start_lines[index]:end_lines[index]]
    # then we throe the line character vectors through the pipeline
    table <- table %>% 
      str_replace_all(":", "") %>%         # remove the ':' at the start
      str_replace_all("\\s\\s*", " " ) %>% # replace all series of whitespace with a single space
      data_frame() %>%
      # split on that space delmiter and give the columns nice names
      separate(1,sep = " ",headers, extra="drop") %>% 
      # add a column for the year this data belongs two
      mutate(year = rep(as.integer(year+index))) %>%
      # most of the data now is chars and groped units (i.e. 1000s pounds etc)
      # the gsub calls are needed to prevent the ","s from messing up as.numeric
      mutate(numcolonies = as.numeric(gsub(",","",numcolonies))*1000) %>%
      mutate(yieldpercol = as.numeric(gsub(",","",yieldpercol))) %>%
      mutate(production = as.numeric(gsub(",","",production))*1000) %>%
      mutate(stocks = as.numeric(gsub(",","",stocks))*1000) %>%
      mutate(avgprice = as.numeric(gsub(",","",avgprice))/100) %>%
      mutate(value = as.numeric(gsub(",","",value))*1000)
    
      
    # here I rearrange the columns such that the year value comes right after the state.abb
    # I just do this because I think it makes it look slightly better.
    table <- table[c(1,8,2,3,4,5,6,7)]
    
    # iteratively rbind the current years table to the accumulator dataframe
    df = rbind(df,table)
  }
  #return the accumlated data frame
  df
}
```

Now its time to read in the text files and let the function do its job.  
Note: The year parameters passed in are 1 year less than the first year held withi the file. this is becuase the indexing in the function above starts at 1.


```{r dataframes}

honey_1986_1992 <- read_lines ("HoneyData_1986-1992.txt")
honey_1993_1997 <- read_lines ("HoneyData_1993-1997.txt")
honey_1998_2002 <- read_lines ("HoneyData_1998-2002.txt")
honey_2003_2007 <- read_lines ("HoneyData_2003-2007.txt")


masterhoneydf = NULL

masterhoneydf <- masterhoneydf %>%
  rbind(gethoneydatav1(honey_1986_1992,1985)) %>%
  rbind(gethoneydatav1(honey_1993_1997,1992)) %>%
  rbind(gethoneydatav1(honey_1998_2002,1997)) %>%
  rbind(gethoneydatav1(honey_2003_2007,2002))

honey_1986_1992df <- gethoneydatav1(honey_1986_1992,1985)
honey_1993_1997df <- gethoneydatav1(honey_1993_1997,1992)
honey_1998_2002df <- gethoneydatav1(honey_1998_2002,1997)
honey_2003_2007df <- gethoneydatav1(honey_2003_2007,2002)



```




**Scraping Data from 2008-2012**  
The methodology of scraping the data will be basically the same but this time instead of state abreviations being the start of the tables it is the full state names. There is also different unecessary characters withing the table rows that we will need to remove. I have included a link to this file in particular below for conveince 

[HoneyData_2008-2012.txt](./HoneyData_2008-2012.txt)

since there is only one file we wont need to make a function we can just use a pipeline.
There are two problems with the tables in this file
  - some state names have spaces so we need to be careful when splitting on spaces
  - there are some blank lines in the tables that need to be removed
```{r anotherone}

text <- read_lines("HoneyData_2008-2012.txt")

# " ." added for increased accuracy
start_lines <- c(grep("Alabama .", text)) 
# all the tables in this doc end with an "Other States" row
othersts_lines <- c(grep("Other States",text))  

honey_2008_2012df <- NULL
for (index in 1:length(start_lines)){
    
    # since we have index bounds of the table we can grab the lines that make up the table
    table <- text[start_lines[index]:othersts_lines[index]]
    # then we throw the line character vectors through the pipeline
    table <- table %>% 
      str_replace_all("\\.", "") %>%       # remove all '.', remember to escape it
      str_replace_all("\\s\\s*", " " ) %>% # replace all series of whitespace with a single space
      data_frame() %>%
      separate(1,sep = " : ",c("state","data"), extra="drop") %>% # separate first colunm on " : "
      # we already have the state columns so we dont need that element I headers hence the '-1'
      separate(2,sep = " ",headers[-1], extra="drop") %>% # separate second colmn on spaces
      rbind(c("Alaska",rep("0",7))) %>% # adding a zero line for alaska
      drop_na() %>% # drop columns that have na which are a few empty lines from the table
      # most of the data now is chars and groped units (i.e. 1000s pounds etc)
      # the gsub calls are needed to prevent the ","s from messing up as.numeric
      mutate(numcolonies = as.numeric(gsub(",","",numcolonies))*1000) %>%
      mutate(yieldpercol = as.numeric(gsub(",","",yieldpercol))) %>%
      mutate(production = as.numeric(gsub(",","",production))*1000) %>%
      mutate(stocks = as.numeric(gsub(",","",stocks))*1000) %>%
      mutate(avgprice = as.numeric(gsub(",","",avgprice))/100) %>%
      mutate(value = as.numeric(gsub(",","",value))*1000)
    
    
    # figure out which states are not present
    states_not_present <- state.name[! state.name %in% table$state]
    # find the number of states that are not present
    num_states_not_present <- length(states_not_present)
    # extract the aggregated other states  table values without the bogus state name
    other_state_vals <- subset(table, str_detect(table$state,"States"))[-1]
    # we only want the data colunms so we can drop the first column
    # and then we want the other cols to be divided by the number of states they are summed from
    # except for yieldpercol and avg price
    other_state_vals[1,1] <- round(other_state_vals[1,1]/num_states_not_present)
    other_state_vals[1,3] <- round(other_state_vals[1,3]/num_states_not_present)
    other_state_vals[1,4] <- round(other_state_vals[1,4]/num_states_not_present)
    other_state_vals[1,6] <- round(other_state_vals[1,6]/num_states_not_present)
    
    other <- cbind(states_not_present,other_state_vals)
    colnames(other)[1] <- "state"
  
    table <- table %>% rbind(other) %>%
      filter(!str_detect(state,"State")) %>%
      # add a column for the year this data belongs two
      mutate(year = rep(as.integer(2007+index)))

    
    # # here I rearrange the columns such that the year value comes right after the state.abb
    # # I just do this because I think it makes it look slightly better.
    table <- table[c(1,8,2,3,4,5,6,7)]
    # 
    # # iteratively rbind the current years table to the accumulator dataframe
    honey_2008_2012df = rbind(honey_2008_2012df,table)
  }
honey_2008_2012df

test <- state.name[! state.name %in% table$state]  
test

```


### U.S. Honey Production Data 1986 - 2012
The combined Honey Production Data.  

```{r, result='asis', echo=FALSE}
datatable(masterhoneydf,filter="top",style="bootstrap")
```



## Exploratory Data Analysis
lets make some nice plots that visualize some of the data we scraped.



### plot 1 : UNTITLED

```{r plot1}
masterhoneydf %>%
  ggplot(aes(x=year,y=numcolonies,group=factor(state))) +
  # color is a particular hex value for "honey" I found online
  geom_line(color="#f9c901", alpha=3/4, size=3/4) + 
  labs(title="Number of Honey Colonies in States over Time",
          x="Year", y="Number of Colonies")

```

another chart that shows the change in production

### plot 2 : UNTITLED

```{r plot2v1}
masterhoneydf %>%
  group_by(state) %>%
  summarize(avg = mean(numcolonies)) %>%
  arrange(avg) %>%
  ggplot(aes(x=state,y=avg)) +
  geom_bar(stat="identity",fill="#f9c901") +
  labs(title="Number of Honey Colonies in Each States ",
          x="State", y="Number of Colonies")

```

**Plot 2 Continued**  

lets revise the r code to show the values in order using `reorder`.

```{r plot2v2}
masterhoneydf %>%
  group_by(state) %>%
  summarize(avg_numcol = mean(numcolonies)) %>%
  ggplot(aes(x=reorder(state,-avg_numcol),y=avg_numcol)) +
  geom_bar(stat="identity",fill="#f9c901") +
  labs(title="Number of Honey Colonies in Each States ",
          x="State", y="Number of Colonies")

```


### plot 3 : UNTITLED

```{r fiftstates}
library(fiftystater)

crimes <- data.frame(state = tolower(rownames(USArrests)), USArrests)

# map_id creates the aesthetic mapping to the state name column in your data
p <- ggplot(crimes, aes(map_id = state)) + 
  # map points to the fifty_states shape data
  geom_map(aes(fill = Assault), map = fifty_states) + 
  expand_limits(x = fifty_states$long, y = fifty_states$lat) +
  coord_map() 
  
masterhoneydf %>%
  group_by(state) %>%
  summarize(avg_numcol = mean(numcolonies)) %>%

ggplot(aes(map_id = str_to_lower(abbr_to_name[state]))) + 
  # map points to the fifty_states shape data
  geom_map(aes(fill = avg_numcol), map = fifty_states) + 
  expand_limits(x = fifty_states$long, y = fifty_states$lat) +
  scale_fill_gradient(low="#ffffe5", high= "#f9c901", guide = "colorbar", name = "Number of Colonies",labels = NULL) +
  coord_map() +
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = NULL) +
  labs(x = "", y = "") +
  theme(legend.position = "bottom",
        panel.background = element_blank())




```

facet every five years

what do these plots show us?
are there any peculirarites in the data that we need to address?
and interesting insights that may have been discovered

## Hyothesis Testing
My hypothesis is I will probably not finish this in time


##Machine Learning
In this section I will show how to create a prediction with the given data. Particularly, the model will predict the whether the change of number of colonies will go up or down. In this paritcular example we will try to predict the change in number of colonies in 2012 using all the data prior to 2012 (1986-2011). In order to do this we will need to create a data frame that has the result we are looking for.

###Setup
This new outcome dataframe needs to...
```{r outcome}

```


###Creating the Model 
I will make use of a random forest to make the model...
```{r}

```




###Testing
To test this model we would use a k-fold valdidation technique. Unfortunately I was unable to complete this section in time to submit the project Im sorry to let you down. If you like to learn more about k-fold validiation I recommend checking out this site and this set of lecture slides from my Intro to data science course

##Conclusion

`This was nice wasnt it.`
We learned how to scrape data from text files, how to handle missing data. We saw how to make graphs that can intuitively show the data and how attributes relate to one another. We were also able to make a map that that can show the data geographically which is not always possible in some datasets

